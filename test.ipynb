{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n",
      "File \u001b[0;32m~/Downloads/ltm_test/venv/lib/python3.11/site-packages/langchain/llms/__init__.py:545\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llms\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;66;03m# If not in interactive env, raise warning.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive_env():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import google.generativeai as genai\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# if not api_key:\n",
    "#     raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
    "\n",
    "# genai.configure(api_key=api_key)\n",
    "\n",
    "# class GeminiLLM:\n",
    "#     def __init__(self, model_name=\"gemini-1.5-flash\"):\n",
    "#         self.model = genai.GenerativeModel(model_name)\n",
    "\n",
    "#     def call_api(self, prompt):\n",
    "#         try:\n",
    "#             response = self.model.generate_content(prompt)\n",
    "#             return response.text\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error calling Gemini API: {e}\")\n",
    "#             return None\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "class LLaMA3LLM:\n",
    "    def __init__(self, model_name=\"llama-2\"):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def call_api(self, prompt):\n",
    "        try:\n",
    "            # Call the Ollama CLI with the prompt using a subprocess\n",
    "            command = [\"ollama\", \"generate\", \"--model\", self.model_name, \"--prompt\", prompt]\n",
    "            result = subprocess.run(command, capture_output=True, text=True)\n",
    "            \n",
    "            # Parse the result\n",
    "            response_text = result.stdout.strip()\n",
    "            print(response_text)\n",
    "            return response_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling LLaMA API: {e}\")\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "llm = LLaMA3LLM(model_name=\"llama-3\")\n",
    "response = llm.call_api(\"What is Takayasu Arteritis?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/zp_8cq8s4418zg3_64l36h780000gn/T/ipykernel_68420/2931177681.py:6: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  ollama_llm(prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Life is a beautiful and complex journey full of ups and downs. It's the sum of our experiences, relationships, and memories.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "\n",
    "ollama_llm = Ollama(model= 'llama2')\n",
    "prompt = \"\"\"<s>[INST] <<SYS>>\\n{You are a virtual assistant who asnwers user question with short and sweet sentences}\\n<</SYS>>\\n\\n{what is life?} [/INST]\"\"\"\n",
    "ollama_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\n",
    "respond in short sentence within 20 words\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "ollama_llm = Ollama(model= 'llama2')\n",
    "chain = prompt | ollama_llm\n",
    "response = chain.invoke({\"question\": \"What is life?\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Life is a mysterious and complex journey, full of ups and downs, growth and learning.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_groq\n",
      "  Downloading langchain_groq-0.2.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
      "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in ./venv/lib/python3.11/site-packages (from langchain_groq) (0.3.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.6.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq<1,>=0.4.1->langchain_groq)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venv/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in ./venv/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (0.1.131)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./venv/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./venv/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (8.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in ./venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (2.2.3)\n",
      "Downloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
      "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, groq, langchain_groq\n",
      "Successfully installed distro-1.9.0 groq-0.11.0 langchain_groq-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "import openai\n",
    "llm = ChatGroq(\n",
    "    api_key=api_key,\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! How can I help you today? If you have any questions about a specific topic or just want to chat, I'm here for you.\\n\\nSome popular topics include:\\n\\n* Programming and software development\\n* Data science and machine learning\\n* Technology and the latest gadgets\\n* Productivity and time management\\n* Learning and education\\n* Health and wellness\\n* Personal finance and investing\\n* Travel and adventure\\n* And much more!\\n\\nJust let me know what's on your mind and I'll do my best to assist you. I'm here to help and make your day a little bit easier. ðŸ˜Š\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 9, 'total_tokens': 147, 'completion_time': 0.220049267, 'prompt_time': 0.001824477, 'queue_time': 0.011814542, 'total_time': 0.221873744}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-d698b3d9-e008-4019-9e91-d10a7d592278-0', usage_metadata={'input_tokens': 9, 'output_tokens': 138, 'total_tokens': 147})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Takayasu Arteritis is a rare, chronic inflammatory disease that primarily affects the aorta and its major branches. It is also known as pulseless disease or aortic arch syndrome. The condition is more common in women, particularly those of Asian descent, and typically occurs between the ages of 10 and 40.\\n\\nThe exact cause of Takayasu Arteritis is unknown, but it is thought to be an autoimmune disorder, in which the body's immune system mistakenly attacks its own blood vessels. This can lead to inflammation, thickening, and narrowing of the artery walls, which can reduce blood flow to various parts of the body.\\n\\nSymptoms of Takayasu Arteritis can vary widely, depending on which blood vessels are affected and the severity of the blockage. Common symptoms include:\\n\\n* Fatigue\\n* Headaches\\n* Dizziness or fainting\\n* Visual disturbances\\n* Upper body pain or discomfort\\n* Weak or absent pulses in the arms or legs\\n* Cool or pale skin in the extremities\\n* High blood pressure\\n* Shortness of breath or chest pain\\n\\nDiagnosis of Takayasu Arteritis typically involves a combination of medical history, physical examination, and imaging tests such as angiography, CT scan, or MRI. Treatment usually involves medications to reduce inflammation and suppress the immune system, such as corticosteroids and immunosuppressive drugs. In severe cases, surgery or angioplasty may be necessary to restore blood flow.\\n\\nIf left untreated, Takayasu Arteritis can lead to serious complications such as aneurysms, stroke, or heart attack. However, with early diagnosis and appropriate treatment, most people with Takayasu Arteritis can manage their symptoms and maintain a good quality of life.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 410, 'prompt_tokens': 18, 'total_tokens': 428, 'completion_time': 0.662465133, 'prompt_time': 0.002177145, 'queue_time': 0.011921394, 'total_time': 0.664642278}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-299b4d71-fecf-42c7-9566-8424a82a281f-0', usage_metadata={'input_tokens': 18, 'output_tokens': 410, 'total_tokens': 428})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "api_key = \"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\"\n",
    "def get_groq_llm(model_name:str=\"mixtral-8x7b-32768\"):\n",
    "    llm = ChatGroq(api_key=api_key,\n",
    "                          model=\"mixtral-8x7b-32768\",\n",
    "                          temperature=0,\n",
    "                          max_tokens=None,\n",
    "                          timeout=None,\n",
    "                          max_retries=2,)\n",
    "    return llm\n",
    "\n",
    "def query_llm(query:str):\n",
    "    llm = get_groq_llm()\n",
    "    return llm.invoke(query)\n",
    "\n",
    "# Example usage\n",
    "response = query_llm(\"What is Takayasu Arteritis?\")\n",
    "response ## This response contains a response as AIMessage so we need to get only the string part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_to_append = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Takayasu Arteritis is a rare, chronic inflammatory disease that primarily affects the aorta and its major branches. It is also known as pulseless disease or aortic arch syndrome. The condition is more common in women, particularly those of Asian descent, and typically occurs between the ages of 10 and 40.\\n\\nThe exact cause of Takayasu Arteritis is unknown, but it is thought to be an autoimmune disorder, in which the body's immune system mistakenly attacks its own blood vessels. This can lead to inflammation, thickening, and narrowing of the artery walls, which can reduce blood flow to various parts of the body.\\n\\nSymptoms of Takayasu Arteritis can vary widely, depending on which blood vessels are affected and the severity of the blockage. Common symptoms include:\\n\\n* Fatigue\\n* Headaches\\n* Dizziness or fainting\\n* Visual disturbances\\n* Upper body pain or discomfort\\n* Weak or absent pulses in the arms or legs\\n* Cool or pale skin in the extremities\\n* High blood pressure\\n* Shortness of breath or chest pain\\n\\nDiagnosis of Takayasu Arteritis typically involves a combination of medical history, physical examination, and imaging tests such as angiography, CT scan, or MRI. Treatment usually involves medications to reduce inflammation and suppress the immune system, such as corticosteroids and immunosuppressive drugs. In severe cases, surgery or angioplasty may be necessary to restore blood flow.\\n\\nIf left untreated, Takayasu Arteritis can lead to serious complications such as aneurysms, stroke, or heart attack. However, with early diagnosis and appropriate treatment, most people with Takayasu Arteritis can manage their symptoms and maintain a good quality of life.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_to_append ## fetching only string from result_to_append "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/zp_8cq8s4418zg3_64l36h780000gn/T/ipykernel_49783/399592878.py:27: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "/var/folders/n8/zp_8cq8s4418zg3_64l36h780000gn/T/ipykernel_49783/399592878.py:28: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  conversation = LLMChain(\n",
      "/var/folders/n8/zp_8cq8s4418zg3_64l36h780000gn/T/ipykernel_49783/399592878.py:35: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = conversation({\"question\": \"hi\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from gemini_llm import get_groq_llm\n",
    "llm = get_groq_llm()\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    "\n",
    ")\n",
    "response = conversation({\"question\": \"hi\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to chat with you. How can I assist you today? Is there something you would like to talk about or any questions you have in mind? I'm here to help and provide information on a wide range of topics.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! It's nice to chat with you. How can I assist you today? Is there something you would like to talk about or any questions you have in mind? I'm here to help and provide information on a wide range of topics.\", additional_kwargs={}, response_metadata={})]), return_messages=True, memory_key='chat_history')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! It's nice to chat with you. How can I assist you today? Is there something you would like to talk about or any questions you have in mind? I'm here to help and provide information on a wide range of topics.\", additional_kwargs={}, response_metadata={})]), return_messages=True, memory_key='chat_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
